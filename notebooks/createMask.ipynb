{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    import tensorflow as tf\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asf_hyp3 import API\n",
    "import pandas as pd\n",
    "from src.api_functions import hyp3_login, grab_subscription\n",
    "from pprint import pprint\n",
    "from datetime import date, datetime\n",
    "import re\n",
    "from src.model import load_model\n",
    "from src.config import NETWORK_DEMS as dems\n",
    "\n",
    "import dataclasses as dc\n",
    "import numpy as np\n",
    "from dataclasses import field, asdict\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dc.dataclass()\n",
    "class Product:\n",
    "    name: str\n",
    "    granule: str = None\n",
    "    url: str = None\n",
    "    shape: Polygon = None\n",
    "    start: datetime = None\n",
    "    end: datetime = None\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.product_time_regex = re.compile(\n",
    "                r\"S.*1SDV_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})T(?P<start_hour>\\d{2})(\"\n",
    "                r\"?P<start_minute>\\d{2})(?P<start_second>\\d{2})_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})T(\"\n",
    "                r\"?P<end_hour>\\d{2})(?P<end_minute>\\d{2})(?P<end_second>\\d{2})_*\")\n",
    "#         self.start = make_start(name)\n",
    "#         self.end = make_end(name)\n",
    "        self.start = self.make_start(self.granule)\n",
    "        self.end = self.make_end(self.granule)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def make_start(self,product_name) -> datetime:\n",
    "        \n",
    "        regex_match = re.match(self.product_time_regex, product_name)\n",
    "        time_dict = regex_match.groupdict()\n",
    "        for k, v in time_dict.items():\n",
    "            time_dict[k] = int(v)\n",
    "\n",
    "        return datetime(time_dict[\"start_year\"], time_dict[\"start_month\"], time_dict[\"start_day\"],\n",
    "                        time_dict[\"start_hour\"], time_dict[\"start_minute\"], time_dict[\"start_second\"])\n",
    "\n",
    "\n",
    "\n",
    "    def make_end(self, product_name) -> datetime:\n",
    "\n",
    "        regex_match = re.match(self.product_time_regex, product_name)\n",
    "        time_dict = regex_match.groupdict()\n",
    "        for k, v in time_dict.items():\n",
    "            time_dict[k] = int(v)\n",
    "\n",
    "        return datetime(time_dict[\"end_year\"], time_dict[\"end_month\"], time_dict[\"end_day\"],\n",
    "                        time_dict[\"end_hour\"], time_dict[\"end_minute\"], time_dict[\"end_second\"])\n",
    "    \n",
    "    \n",
    "    def to_json(self):\n",
    "        metadata = asdict(self)\n",
    "        metadata['start'] = self.start.isoformat()\n",
    "        metadata['end'] = self.end.isoformat()\n",
    "        metadata['shape'] = str(self.shape)\n",
    "        \n",
    "#         for key in list(metadata):\n",
    "#             if key is datetime:\n",
    "#                 metadata[key] = metadata[key].isoformat()\n",
    "#                 print(f\"TEST: {key}= {metadata[key]}\")\n",
    "        return json.dumps(metadata)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_time(product_name):\n",
    "    product_time_regex = re.compile(\n",
    "        r\"S.*1SDV_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})T(?P<start_hour>\\d{2})(\"\n",
    "        r\"?P<start_minute>\\d{2})(?P<start_second>\\d{2})_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})T(\"\n",
    "        r\"?P<end_hour>\\d{2})(?P<end_minute>\\d{2})(?P<end_second>\\d{2})_*\")\n",
    "\n",
    "    regex_match = re.match(product_time_regex, product_name)\n",
    "    time_dict = regex_match.groupdict()\n",
    "\n",
    "    # converts all dates/times values in dictionary from int to string\n",
    "    for k, v in time_dict.items():\n",
    "        time_dict[k] = int(v)\n",
    "\n",
    "    start = datetime(time_dict[\"start_year\"], time_dict[\"start_month\"], time_dict[\"start_day\"],\n",
    "                     time_dict[\"start_hour\"], time_dict[\"start_minute\"], time_dict[\"start_second\"])\n",
    "\n",
    "    end = datetime(time_dict[\"end_year\"], time_dict[\"end_month\"], time_dict[\"end_day\"],\n",
    "                   time_dict[\"end_hour\"], time_dict[\"end_minute\"], time_dict[\"end_second\"])\n",
    "\n",
    "\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_in_time_bounds(product_name, start, end):\n",
    "    prod_start, prod_end = product_time(product_name)\n",
    "    \n",
    "    return prod_start > start and prod_end < end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_products(api, sub_id, start, end):\n",
    "    \n",
    "    response = api.get_products(sub_id=sub_id)\n",
    "    \n",
    "    products = []\n",
    "    for product in response:\n",
    "        if product_in_time_bounds(product['granule'], start, end):\n",
    "            products.append(Product(product['name'],\n",
    "                                    product['granule'],\n",
    "                                    product['url']\n",
    "                                   )\n",
    "                           )\n",
    "    return products\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_tile_image(\n",
    "\n",
    "        image: np.ndarray, width: int = dems, height: int = dems\n",
    ") -> np.ndarray:\n",
    "    _nrows, _ncols = image.shape\n",
    "    _strides = image.strides\n",
    "\n",
    "    nrows, _m = divmod(_nrows, height)\n",
    "    ncols, _n = divmod(_ncols, width)\n",
    "\n",
    "    assert _m == 0, \"Image must be evenly tileable. Please pad it first\"\n",
    "    assert _n == 0, \"Image must be evenly tileable. Please pad it first\"\n",
    "\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        np.ravel(image),\n",
    "        shape=(nrows, ncols, height, width),\n",
    "        strides=(height * _strides[0], width * _strides[1], *_strides),\n",
    "        writeable=False\n",
    "    ).reshape(nrows * ncols, height, width)\n",
    "\n",
    "\n",
    "def get_tile_dimensions(height: int, width: int, tile_size: int):\n",
    "    return int(np.ceil(height / tile_size)), int(np.ceil(width / tile_size))\n",
    "\n",
    "\n",
    "def write_mask_to_file(\n",
    "        mask: np.ndarray, file_name: str, projection: str, geo_transform: str\n",
    ") -> None:\n",
    "    (width, height) = mask.shape\n",
    "    out_image = gdal.GetDriverByName('GTiff').Create(\n",
    "        file_name, height, width, bands=1\n",
    "    )\n",
    "    out_image.SetProjection(projection)\n",
    "    out_image.SetGeoTransform(geo_transform)\n",
    "    out_image.GetRasterBand(1).WriteArray(mask)\n",
    "    out_image.GetRasterBand(1).SetNoDataValue(0)\n",
    "    out_image.FlushCache()\n",
    "\n",
    "\n",
    "def pad_image(image: np.ndarray, to: int) -> np.ndarray:\n",
    "    height, width = image.shape\n",
    "\n",
    "    n_rows, n_cols = get_tile_dimensions(height, width, to)\n",
    "    new_height = n_rows * to\n",
    "    new_width = n_cols * to\n",
    "\n",
    "    padded = np.zeros((new_height, new_width))\n",
    "    padded[:image.shape[0], :image.shape[1]] = image\n",
    "    return padded\n",
    "\n",
    "\n",
    "# TODO: Cut edge fill on final mask (make it more pretty!\n",
    "# TODO: FIX VV/VH ISSUE. ONLY WORKS WITH VV RIGHT NOW!\n",
    "# TODO: Split get vv/vh tiles into functions\n",
    "# TODO: Try differnt tiling method (not strided)\n",
    "def create_water_mask(\n",
    "        model_path: str, vv_path: str, vh_path: str, outfile: str, verbose: int = 0\n",
    "):\n",
    "    if not os.path.isfile(vv_path):\n",
    "        raise FileNotFoundError(f\"Tiff '{vv_path}' does not exist\")\n",
    "\n",
    "    if not os.path.isfile(vh_path):\n",
    "        raise FileNotFoundError(f\"Tiff '{vh_path}' does not exist\")\n",
    "\n",
    "    def get_tiles(img_path):\n",
    "        f = gdal.Open(img_path)\n",
    "        img_array = f.ReadAsArray()\n",
    "        original_shape = img_array.shape\n",
    "        n_rows, n_cols = get_tile_dimensions(*original_shape, tile_size=dems)\n",
    "        padded_img_array = pad_image(img_array, dems)\n",
    "        invalid_pixels = np.nonzero(padded_img_array == 0.0)\n",
    "        img_tiles = stride_tile_image(padded_img_array)\n",
    "        return img_tiles, n_rows, n_cols, invalid_pixels, f.GetProjection(), f.GetGeoTransform()\n",
    "\n",
    "    # Get vv tiles\n",
    "    vv_tiles, vv_rows, vv_cols, vv_pixels, vv_projection, vv_transform = get_tiles(vv_path)\n",
    "\n",
    "    # Get vh tiles\n",
    "    vh_tiles, vh_rows, vh_cols, vh_pixels, vh_projection, vh_transform = get_tiles(vh_path)\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Predict masks\n",
    "    masks = model.predict(\n",
    "        np.stack((vv_tiles, vh_tiles), axis=3), batch_size=1, verbose=verbose\n",
    "    )\n",
    "\n",
    "    masks.round(decimals=0, out=masks)\n",
    "\n",
    "    # Stitch masks together\n",
    "    mask = masks.reshape((vv_rows, vv_cols, dems, dems)) \\\n",
    "        .swapaxes(1, 2) \\\n",
    "        .reshape(vv_rows * dems, vv_cols * dems)  # yapf: disable\n",
    "\n",
    "    mask[vv_pixels] = 0\n",
    "    write_mask_to_file(mask, outfile, vv_projection, vv_transform)\n",
    "\n",
    "    # Needed?\n",
    "    f = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_from_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_product(product_path, output_dir):\n",
    "    \"\"\"Extract vv and vh tifs from product\"\"\"\n",
    "    product_name = Path(product_path).stem\n",
    "    sar_regex = re.compile(r\"(S1[A|B])_(.{2})_(.*)_(VV|VH)(.tif)\")\n",
    "\n",
    "    with zipfile.ZipFile(product_path, \"r\") as zip_ref:\n",
    "        with TemporaryDirectory() as tmpdir_name:\n",
    "            for file_info in zip_ref.infolist():\n",
    "                if re.fullmatch(sar_regex,file_info.filename):\n",
    "                    zip_ref.extract(file_info,path=tmpdir_name)\n",
    "                    shutil.move(f\"{tmpdir_name}/{file_info.filename}\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_water_masks(model, products: list, name: str):\n",
    "    for product in products:\n",
    "        with TemporaryDirectory() as tmpdir_name:\n",
    "            extract_from_product(product)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " login successful!\n",
      " Welcome jmherning\n",
      "[Product(name='S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9.zip', granule='S1A_IW_GRDH_1SDV_20191203T224518_20191203T224543_030190_037348_297D', url='https://hyp3-download.asf.alaska.edu/asf/data/S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9.zip', shape=None, start=datetime.datetime(2019, 12, 3, 22, 45, 18), end=datetime.datetime(2019, 12, 3, 22, 45, 43)),\n",
      " Product(name='S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53.zip', granule='S1A_IW_GRDH_1SDV_20191203T224543_20191203T224608_030190_037348_8C67', url='https://hyp3-download.asf.alaska.edu/asf/data/S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53.zip', shape=None, start=datetime.datetime(2019, 12, 3, 22, 45, 43), end=datetime.datetime(2019, 12, 3, 22, 46, 8))]\n"
     ]
    }
   ],
   "source": [
    "start = datetime(year=2019,month=12,day=3,hour=0,minute=0,second=0)\n",
    "end   = datetime(year=2019,month=12,day=4,hour=0,minute=0,second=0)\n",
    "\n",
    "api = hyp3_login()\n",
    "# subscription = grab_subscription(api)\n",
    "# sub_id = subscription['id']\n",
    "id = 2810\n",
    "\n",
    "\n",
    "products = get_sub_products(api, id, start, end)\n",
    "pprint(products)\n",
    "\n",
    "# response = api.get_products(sub_id = id)\n",
    "# pprint(response)\n",
    "# products_list = [product['granule'] for product in response]\n",
    "\n",
    "# products = [p for p in products_list if product_in_time_bounds(p, start, end)]\n",
    "\n",
    "# # products = get_sub_granules(api, sub_id, start, end)\n",
    "\n",
    "# print(f\"product count is {len(products)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
