{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asf_hyp3 import API\n",
    "from collections import namedtuple\n",
    "# import pandas as pd\n",
    "from src.api_functions import hyp3_login, grab_subscription\n",
    "from pprint import pprint\n",
    "from datetime import date, datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import dataclasses as dc\n",
    "import numpy as np\n",
    "from dataclasses import field, asdict\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "from itertools import groupby\n",
    "import os\n",
    "import itertools\n",
    "from osgeo import gdal\n",
    "from src.model import load_model\n",
    "from src.config import NETWORK_DEMS as dems\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dc.dataclass()\n",
    "class Product:\n",
    "    name: str\n",
    "    granule: str = None\n",
    "    url: str = None\n",
    "    shape: Polygon = None\n",
    "    start: datetime = None\n",
    "    end: datetime = None\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.product_time_regex = re.compile(\n",
    "                r\"S.*1SDV_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})T(?P<start_hour>\\d{2})(\"\n",
    "                r\"?P<start_minute>\\d{2})(?P<start_second>\\d{2})_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})T(\"\n",
    "                r\"?P<end_hour>\\d{2})(?P<end_minute>\\d{2})(?P<end_second>\\d{2})_*\")\n",
    "#         self.start = make_start(name)\n",
    "#         self.end = make_end(name)\n",
    "        self.start = self._make_start(self.granule)\n",
    "        self.end = self._make_end(self.granule)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def _make_start(self,product_name) -> datetime:\n",
    "        \n",
    "        regex_match = re.match(self.product_time_regex, product_name)\n",
    "        time_dict = regex_match.groupdict()\n",
    "        for k, v in time_dict.items():\n",
    "            time_dict[k] = int(v)\n",
    "\n",
    "        return datetime(time_dict[\"start_year\"], time_dict[\"start_month\"], time_dict[\"start_day\"],\n",
    "                        time_dict[\"start_hour\"], time_dict[\"start_minute\"], time_dict[\"start_second\"])\n",
    "\n",
    "\n",
    "\n",
    "    def _make_end(self, product_name) -> datetime:\n",
    "\n",
    "        regex_match = re.match(self.product_time_regex, product_name)\n",
    "        time_dict = regex_match.groupdict()\n",
    "        for k, v in time_dict.items():\n",
    "            time_dict[k] = int(v)\n",
    "\n",
    "        return datetime(time_dict[\"end_year\"], time_dict[\"end_month\"], time_dict[\"end_day\"],\n",
    "                        time_dict[\"end_hour\"], time_dict[\"end_minute\"], time_dict[\"end_second\"])\n",
    "    \n",
    "    \n",
    "    def to_json(self):\n",
    "        metadata = asdict(self)\n",
    "        metadata['start'] = self.start.isoformat()\n",
    "        metadata['end'] = self.end.isoformat()\n",
    "        metadata['shape'] = str(self.shape)\n",
    "        \n",
    "#         for key in list(metadata):\n",
    "#             if key is datetime:\n",
    "#                 metadata[key] = metadata[key].isoformat()\n",
    "#                 print(f\"TEST: {key}= {metadata[key]}\")\n",
    "        return json.dumps(metadata)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials namedtup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = namedtuple('credentials', 'username password')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_time(product_name):\n",
    "    product_time_regex = re.compile(\n",
    "        r\"S.*1SDV_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})T(?P<start_hour>\\d{2})(\"\n",
    "        r\"?P<start_minute>\\d{2})(?P<start_second>\\d{2})_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})T(\"\n",
    "        r\"?P<end_hour>\\d{2})(?P<end_minute>\\d{2})(?P<end_second>\\d{2})_*\")\n",
    "\n",
    "    regex_match = re.match(product_time_regex, product_name)\n",
    "    time_dict = regex_match.groupdict()\n",
    "\n",
    "    # converts all dates/times values in dictionary from int to string\n",
    "    for k, v in time_dict.items():\n",
    "        time_dict[k] = int(v)\n",
    "\n",
    "    start = datetime(time_dict[\"start_year\"], time_dict[\"start_month\"], time_dict[\"start_day\"],\n",
    "                     time_dict[\"start_hour\"], time_dict[\"start_minute\"], time_dict[\"start_second\"])\n",
    "\n",
    "    end = datetime(time_dict[\"end_year\"], time_dict[\"end_month\"], time_dict[\"end_day\"],\n",
    "                   time_dict[\"end_hour\"], time_dict[\"end_minute\"], time_dict[\"end_second\"])\n",
    "\n",
    "\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_in_time_bounds(product_name, start, end):\n",
    "    prod_start, prod_end = product_time(product_name)\n",
    "    \n",
    "    return prod_start > start and prod_end < end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_products(api, sub_id, start, end):\n",
    "    \n",
    "    response = api.get_products(sub_id=sub_id)\n",
    "    \n",
    "    products = []\n",
    "    for product in response:\n",
    "        if product_in_time_bounds(product['granule'], start, end):\n",
    "            products.append(Product(product['name'],\n",
    "                                    product['granule'],\n",
    "                                    product['url']\n",
    "                                   )\n",
    "                           )\n",
    "    return products\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tiles(input_tif: str, output_directory: str, tile_size: int, name: str):\n",
    "    \"\"\"Creates tiles of given dimension out of input tif file\"\"\"\n",
    "    input_image = gdal.Open(input_tif)\n",
    "\n",
    "    array = input_image.ReadAsArray()\n",
    "\n",
    "    rows, cols = array.shape\n",
    "\n",
    "    tile_indexes = itertools.product(\n",
    "        range(0, rows, tile_size), range(0, cols, tile_size))\n",
    "\n",
    "    for (row, col) in tile_indexes:\n",
    "        in_bounds = row + tile_size < rows and col + tile_size < cols\n",
    "        if in_bounds:\n",
    "            gdal.Translate(\n",
    "                f'{output_directory}{name}_x{row / tile_size}y{col / tile_size}.tif',\n",
    "                input_tif,\n",
    "                srcWin=[row, col, tile_size, tile_size],\n",
    "                format=\"GTiff\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stride_tile_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_tile_image(\n",
    "\n",
    "        image: np.ndarray, width: int = dems, height: int = dems\n",
    ") -> np.ndarray:\n",
    "    _nrows, _ncols = image.shape\n",
    "    _strides = image.strides\n",
    "\n",
    "    nrows, _m = divmod(_nrows, height)\n",
    "    ncols, _n = divmod(_ncols, width)\n",
    "\n",
    "    assert _m == 0, \"Image must be evenly tileable. Please pad it first\"\n",
    "    assert _n == 0, \"Image must be evenly tileable. Please pad it first\"\n",
    "\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        np.ravel(image),\n",
    "        shape=(nrows, ncols, height, width),\n",
    "        strides=(height * _strides[0], width * _strides[1], *_strides),\n",
    "        writeable=False\n",
    "    ).reshape(nrows * ncols, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tile_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_dimensions(height: int, width: int, tile_size: int):\n",
    "    return int(np.ceil(height / tile_size)), int(np.ceil(width / tile_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write_mask_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mask_to_file(\n",
    "        mask: np.ndarray, file_name: str, projection: str, geo_transform: str\n",
    ") -> None:\n",
    "    (width, height) = mask.shape\n",
    "    out_image = gdal.GetDriverByName('GTiff').Create(\n",
    "        file_name, height, width, bands=1\n",
    "    )\n",
    "    out_image.SetProjection(projection)\n",
    "    out_image.SetGeoTransform(geo_transform)\n",
    "    out_image.GetRasterBand(1).WriteArray(mask)\n",
    "    out_image.GetRasterBand(1).SetNoDataValue(0)\n",
    "    out_image.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image(image: np.ndarray, to: int) -> np.ndarray:\n",
    "    height, width = image.shape\n",
    "\n",
    "    n_rows, n_cols = get_tile_dimensions(height, width, to)\n",
    "    new_height = n_rows * to\n",
    "    new_width = n_cols * to\n",
    "\n",
    "    padded = np.zeros((new_height, new_width))\n",
    "    padded[:image.shape[0], :image.shape[1]] = image\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_water_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_water_mask(\n",
    "        model_path: str, vv_path: str, vh_path: str, outfile: str, verbose: int = 0\n",
    "):\n",
    "    if not os.path.isfile(vv_path):\n",
    "        raise FileNotFoundError(f\"Tiff '{vv_path}' does not exist\")\n",
    "\n",
    "    if not os.path.isfile(vh_path):\n",
    "        raise FileNotFoundError(f\"Tiff '{vh_path}' does not exist\")\n",
    "\n",
    "    def get_tiles(img_path):\n",
    "        f = gdal.Open(img_path)\n",
    "        img_array = f.ReadAsArray()\n",
    "        original_shape = img_array.shape\n",
    "        n_rows, n_cols = get_tile_dimensions(*original_shape, tile_size=dems)\n",
    "        padded_img_array = pad_image(img_array, dems)\n",
    "        invalid_pixels = np.nonzero(padded_img_array == 0.0)\n",
    "        img_tiles = stride_tile_image(padded_img_array)\n",
    "        return img_tiles, n_rows, n_cols, invalid_pixels, f.GetProjection(), f.GetGeoTransform()\n",
    "\n",
    "    # Get vv tiles\n",
    "    vv_tiles, vv_rows, vv_cols, vv_pixels, vv_projection, vv_transform = get_tiles(vv_path)\n",
    "\n",
    "    # Get vh tiles\n",
    "    vh_tiles, vh_rows, vh_cols, vh_pixels, vh_projection, vh_transform = get_tiles(vh_path)\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Predict masks\n",
    "    masks = model.predict(\n",
    "        np.stack((vv_tiles, vh_tiles), axis=3), batch_size=1, verbose=verbose\n",
    "    )\n",
    "\n",
    "    masks.round(decimals=0, out=masks)\n",
    "\n",
    "    # Stitch masks together\n",
    "    mask = masks.reshape((vv_rows, vv_cols, dems, dems)) \\\n",
    "        .swapaxes(1, 2) \\\n",
    "        .reshape(vv_rows * dems, vv_cols * dems)  # yapf: disable\n",
    "\n",
    "    mask[vv_pixels] = 0\n",
    "    write_mask_to_file(mask, outfile, vv_projection, vv_transform)\n",
    "\n",
    "    # Needed?\n",
    "    f = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_product(product_url: str, save_directory: Path, creds: credentials) -> None:\n",
    "    \"\"\"Download sar product from given url.\"\"\"\n",
    "\n",
    "    filename = product_url.split('/')[-1]\n",
    "    save_path = save_directory / filename\n",
    "\n",
    "    # Authenticate and then get redirect\n",
    "    response_redirect = requests.get(product_url)\n",
    "    redirect_url = response_redirect.url\n",
    "\n",
    "    # authenticate with .netrc and then Download fjle from redirect_url\n",
    "    with requests.get(redirect_url, stream=True, auth=(creds.username, creds.password)) as response:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            print(f\"Downloading {filename}\")\n",
    "\n",
    "            # Downloads files in chucks\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_products(product_urls: list, output_dir: Path, creds: credentials):\n",
    "    for url in product_urls:\n",
    "        download_product(url, output_dir, creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_netrc_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_netrc_credentials() -> credentials:\n",
    "    \"\"\"Returns credentials from .netrc file.\"\"\"\n",
    "\n",
    "    with open('.netrc', 'r') as f:\n",
    "        contents = f.read()\n",
    "    username = contents.split(' ')[3]\n",
    "    password = contents.split(' ')[5].split('\\n')[0]\n",
    "\n",
    "\n",
    "    return credentials(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_from_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_product(product_path, output_dir):\n",
    "    product_name = Path(product_path).stem\n",
    "    sar_regex = re.compile(r\"(S1[A|B])_(.{2})_(.*)_(VV|VH)(.tif)\")\n",
    "\n",
    "    with zipfile.ZipFile(product_path, \"r\") as zip_ref:\n",
    "        with TemporaryDirectory() as tmpdir_name:\n",
    "            for file_info in zip_ref.infolist():\n",
    "                if re.fullmatch(sar_regex,file_info.filename):\n",
    "                    zip_ref.extract(file_info,path=tmpdir_name)\n",
    "                    shutil.move(f\"{tmpdir_name}/{file_info.filename}\", output_dir)\n",
    "                    pprint(f\"moved {file_info.filename} from {tmpdir_name} to {output_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " login successful!\n",
      " Welcome jmherning\n",
      "urls=['https://hyp3-download.asf.alaska.edu/asf/data/S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9.zip', 'https://hyp3-download.asf.alaska.edu/asf/data/S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53.zip']\n"
     ]
    }
   ],
   "source": [
    "start = datetime(year=2019,month=12,day=3,hour=0,minute=0,second=0)\n",
    "end   = datetime(year=2019,month=12,day=4,hour=0,minute=0,second=0)\n",
    "\n",
    "api = hyp3_login()\n",
    "id = 2810\n",
    "\n",
    "\n",
    "products = get_sub_products(api, id, start, end)\n",
    "product_urls = [p.url for p in products]\n",
    "print(f\"urls={product_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this product exist! S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53.zip\n",
      "this product exist! S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9.zip\n"
     ]
    }
   ],
   "source": [
    "input_directory   = Path.cwd() / \"data\"/ \"input\"\n",
    "working_directory = Path.cwd() / \"data\"/ \"working\"\n",
    "output_directory  = Path.cwd() / \"data\"/ \"output\"\n",
    "\n",
    "ed_creds = get_netrc_credentials()\n",
    "\n",
    "\n",
    "# Uncoment this to download again!\n",
    "# download_products(product_urls, input_directory, ed_creds)\n",
    "for f in input_directory.glob(\"*.zip\"):\n",
    "    print(f\"this product exist! {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract vv/vh tifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('moved '\n",
      " 'S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53/S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53_VH.tif '\n",
      " 'from /tmp/tmp7lalua5_ to '\n",
      " '/home/jmherning/code/AI_Water/notebooks/data/working')\n",
      "('moved '\n",
      " 'S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53/S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53_VV.tif '\n",
      " 'from /tmp/tmp7lalua5_ to '\n",
      " '/home/jmherning/code/AI_Water/notebooks/data/working')\n",
      "('moved '\n",
      " 'S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9/S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9_VV.tif '\n",
      " 'from /tmp/tmpf681jqpj to '\n",
      " '/home/jmherning/code/AI_Water/notebooks/data/working')\n",
      "('moved '\n",
      " 'S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9/S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9_VH.tif '\n",
      " 'from /tmp/tmpf681jqpj to '\n",
      " '/home/jmherning/code/AI_Water/notebooks/data/working')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'working'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-7b1f9c8fedb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mextract_from_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworking_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{os.listdir(working_directory.name)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'working'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for f in input_directory.glob(\"*.zip\"):\n",
    "    prod_path = input_directory / f.name\n",
    "    extract_from_product(prod_path, working_directory)\n",
    "\n",
    "print(f\"{os.listdir(str(working_directory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vvvh_set(vh=PosixPath('/home/jmherning/code/AI_Water/notebooks/data/working/S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9_VH.tif'), vv=PosixPath('/home/jmherning/code/AI_Water/notebooks/data/working/S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9_VV.tif')), vvvh_set(vh=PosixPath('/home/jmherning/code/AI_Water/notebooks/data/working/S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53_VH.tif'), vv=PosixPath('/home/jmherning/code/AI_Water/notebooks/data/working/S1A_IW_20191203T224543_DVP_RTC10_G_gpuned_3A53_VV.tif'))]\n"
     ]
    }
   ],
   "source": [
    "TYPE_REGEX = re.compile(r\"(.*)_(VV|VH).(tiff|tif|TIFF|TIF)\")\n",
    "\n",
    "vvvh_set = namedtuple('vvvh_set', ['vh', 'vv'])\n",
    "\n",
    "def get_sar_paths(directory_path: str) -> list:\n",
    "    \"\"\"returns a list of namedTuples (sar_sets) that store vv, vh, and mask paths\"\"\"\n",
    "    dataset_path = Path(directory_path)\n",
    "\n",
    "    path_generator = dataset_path.rglob('*.tif')\n",
    "    paths = sorted([path for path in path_generator if path.is_file()])\n",
    "    return [vvvh_set(*g) for k, g in groupby(paths, key=lambda path: re.match(TYPE_REGEX, path.name)[1])]\n",
    "\n",
    "\n",
    "\n",
    "vvvh_paths = get_sar_paths(str(working_directory))\n",
    "print(sar_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run: Create Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = datetime(year=2019,month=12,day=3,hour=0,minute=0,second=0)\n",
    "# end   = datetime(year=2019,month=12,day=4,hour=0,minute=0,second=0)\n",
    "\n",
    "# api = hyp3_login()\n",
    "# id = 2810\n",
    "\n",
    "model = 'AI_MEKONG_CONSOLIDATED_FCN_512'\n",
    "# mask_name = \"Mekong_consolidated_test_notebook_dellater\"\n",
    "output_path = output_directory.relative_to(Path.cwd())\n",
    "cnt = 1\n",
    "for s in vvvh_paths:\n",
    "    vv_path = Path(s.vv).relative_to(Path.cwd())\n",
    "    vh_path = Path(s.vh).relative_to(Path.cwd())\n",
    "    out = output_path / f\"{vv_path.stem}_{cnt}.tif\"\n",
    "    create_water_mask(model, str(vv_path), str(vh_path), str(out))\n",
    "    cnt += 1\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# products = get_sub_products(api, id, start, end)\n",
    "# product_urls = [p.url for p in products]\n",
    "# pprint(f\"urls to download={product_urls}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/working/S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9_VV.tif')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('/home/jmherning/code/AI_Water/notebooks/data/working/S1A_IW_20191203T224518_DVP_RTC10_G_gpuned_54B9_VV.tif').relative_to('/home/jmherning/code/AI_Water/notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
